{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]"},"docs":[{"location":"","text":"Homemade documentation (Ops oriented) for personal use","title":"Home"},{"location":"about/","text":"About Administrateur Syst\u00e8me OPS dans la r\u00e9gion rennaise.","title":"About"},{"location":"about/#about","text":"Administrateur Syst\u00e8me OPS dans la r\u00e9gion rennaise.","title":"About"},{"location":"tags/","text":"Tags Following is a list of relevant tags: Bash Start/Stop VM via Api Vmware Git Deploy Hugo webpages on Github Deploy mkdocs webpages on Github Github Deploy Hugo webpages on Github Deploy mkdocs webpages on Github Gitlab Gitlab configuration LXC PiHole Linux Linux manage disk Linux manage network Proxmox NFS Proxmox packer Monitoring Limiter les appels de l'auto discovery Zabbix NFS Proxmox NFS Network Linux manage network Partition Linux manage disk PiHole PiHole Proxmox Proxmox NFS Proxmox packer VMWmare Start/Stop VM via Api Vmware Zabbix Limiter les appels de l'auto discovery Zabbix goHugo Deploy Hugo webpages on Github mkdocs Deploy mkdocs webpages on Github packer Proxmox packer","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#bash","text":"Start/Stop VM via Api Vmware","title":"Bash"},{"location":"tags/#git","text":"Deploy Hugo webpages on Github Deploy mkdocs webpages on Github","title":"Git"},{"location":"tags/#github","text":"Deploy Hugo webpages on Github Deploy mkdocs webpages on Github","title":"Github"},{"location":"tags/#gitlab","text":"Gitlab configuration","title":"Gitlab"},{"location":"tags/#lxc","text":"PiHole","title":"LXC"},{"location":"tags/#linux","text":"Linux manage disk Linux manage network Proxmox NFS Proxmox packer","title":"Linux"},{"location":"tags/#monitoring","text":"Limiter les appels de l'auto discovery Zabbix","title":"Monitoring"},{"location":"tags/#nfs","text":"Proxmox NFS","title":"NFS"},{"location":"tags/#network","text":"Linux manage network","title":"Network"},{"location":"tags/#partition","text":"Linux manage disk","title":"Partition"},{"location":"tags/#pihole","text":"PiHole","title":"PiHole"},{"location":"tags/#proxmox","text":"Proxmox NFS Proxmox packer","title":"Proxmox"},{"location":"tags/#vmwmare","text":"Start/Stop VM via Api Vmware","title":"VMWmare"},{"location":"tags/#zabbix","text":"Limiter les appels de l'auto discovery Zabbix","title":"Zabbix"},{"location":"tags/#gohugo","text":"Deploy Hugo webpages on Github","title":"goHugo"},{"location":"tags/#mkdocs","text":"Deploy mkdocs webpages on Github","title":"mkdocs"},{"location":"tags/#packer","text":"Proxmox packer","title":"packer"},{"location":"Github/hugo_github/","tags":["goHugo","Github","Git"],"text":"D\u00e9ployer un site goHugo via Github page Pr\u00e9requis Avoir un compte Github Cr\u00e9ation de compte github Installer goHugo sur votre machine locale Installation Hugo Cr\u00e9er 2 repo github Stocker le code hugo (repo priv\u00e9 ou publique) H\u00e9berger les fichiers statiques (repo publique uniquement) Le nom du repo qui h\u00e9bergera le site devra \u00eatre sous la forme: myWebsiteName.github.io qui correspondra par la suite \u00e0 l'url d'acc\u00e8s \u00e0 votre site R\u00e9cup\u00e9rer nos repos en local git clone git@github.com:YourAccountName/YourRepoName.git git clone git@github.com:YourAccountName/myWebsiteName.github.io.git vous pouvez aussi les r\u00e9cup\u00e9rer via https mais les commandes git push [...] vous demanderont un token. Cr\u00e9er un site Hugo Dans votre repo contenant le code (YourRepoName.git) hugo new site myWebsiteName Theme hugo Pour plus de facilit\u00e9 et de rapidit\u00e9, je vous conseille d'utiliser un th\u00e8me maintenu par la communaut\u00e9. R\u00e9f\u00e9rez vous \u00e0 la documentation du theme que vous souhaitez utiliser : themes Git submodule En locale, dans le r\u00e9pertoire principale du site hugo (myWebsiteName) git submodule add -b main https://github.com/YourAccountName/myWebsiteName.github.io.git public Un r\u00e9pertoire nomm\u00e9 public va \u00eatre cr\u00e9\u00e9, tel un lien symbolique vers le repo qui h\u00e9bergera vos fichiers statiques (myWebsiteName.github.io.git). G\u00e9n\u00e9rer les fichiers statiques Hugo Dans le r\u00e9pertoire principe du site hugo (myWebsiteName) hugo -t themeName Les fichiers statiques sont ainsi cr\u00e9\u00e9s dans notre r\u00e9pertoire public. Deployer son site Une fois dans le r\u00e9pertoire public git add . git commit -m \"some commit message\" git push origin main Apr\u00e8s environ 2 minutes (temps de d\u00e9ploiement github variable et consultable via l'url github.com/YourAccountName/myWebsiteName.github.io/deployments/), le site sera maintenant \u00eatre accessible via l'url myWebsiteName.github.io Sauvegarder le reste de son code Dans le r\u00e9pertoire principe du site hugo (myWebsiteName), pousser les modifications comme lors de l'\u00e9tape pr\u00e9c\u00e9dente","title":"goHugo"},{"location":"Github/hugo_github/#deployer-un-site-gohugo-via-github-page","text":"","title":"D\u00e9ployer un site goHugo via Github page"},{"location":"Github/hugo_github/#prerequis","text":"Avoir un compte Github Cr\u00e9ation de compte github Installer goHugo sur votre machine locale Installation Hugo","title":"Pr\u00e9requis"},{"location":"Github/hugo_github/#creer-2-repo-github","text":"Stocker le code hugo (repo priv\u00e9 ou publique) H\u00e9berger les fichiers statiques (repo publique uniquement) Le nom du repo qui h\u00e9bergera le site devra \u00eatre sous la forme: myWebsiteName.github.io qui correspondra par la suite \u00e0 l'url d'acc\u00e8s \u00e0 votre site","title":"Cr\u00e9er 2 repo github"},{"location":"Github/hugo_github/#recuperer-nos-repos-en-local","text":"git clone git@github.com:YourAccountName/YourRepoName.git git clone git@github.com:YourAccountName/myWebsiteName.github.io.git vous pouvez aussi les r\u00e9cup\u00e9rer via https mais les commandes git push [...] vous demanderont un token.","title":"R\u00e9cup\u00e9rer nos repos en local"},{"location":"Github/hugo_github/#creer-un-site-hugo","text":"Dans votre repo contenant le code (YourRepoName.git) hugo new site myWebsiteName","title":"Cr\u00e9er un site Hugo"},{"location":"Github/hugo_github/#theme-hugo","text":"Pour plus de facilit\u00e9 et de rapidit\u00e9, je vous conseille d'utiliser un th\u00e8me maintenu par la communaut\u00e9. R\u00e9f\u00e9rez vous \u00e0 la documentation du theme que vous souhaitez utiliser : themes","title":"Theme hugo"},{"location":"Github/hugo_github/#git-submodule","text":"En locale, dans le r\u00e9pertoire principale du site hugo (myWebsiteName) git submodule add -b main https://github.com/YourAccountName/myWebsiteName.github.io.git public Un r\u00e9pertoire nomm\u00e9 public va \u00eatre cr\u00e9\u00e9, tel un lien symbolique vers le repo qui h\u00e9bergera vos fichiers statiques (myWebsiteName.github.io.git).","title":"Git submodule"},{"location":"Github/hugo_github/#generer-les-fichiers-statiques-hugo","text":"Dans le r\u00e9pertoire principe du site hugo (myWebsiteName) hugo -t themeName Les fichiers statiques sont ainsi cr\u00e9\u00e9s dans notre r\u00e9pertoire public.","title":"G\u00e9n\u00e9rer les fichiers statiques Hugo"},{"location":"Github/hugo_github/#deployer-son-site","text":"Une fois dans le r\u00e9pertoire public git add . git commit -m \"some commit message\" git push origin main Apr\u00e8s environ 2 minutes (temps de d\u00e9ploiement github variable et consultable via l'url github.com/YourAccountName/myWebsiteName.github.io/deployments/), le site sera maintenant \u00eatre accessible via l'url myWebsiteName.github.io","title":"Deployer son site"},{"location":"Github/hugo_github/#sauvegarder-le-reste-de-son-code","text":"Dans le r\u00e9pertoire principe du site hugo (myWebsiteName), pousser les modifications comme lors de l'\u00e9tape pr\u00e9c\u00e9dente","title":"Sauvegarder le reste de son code"},{"location":"Github/mkdocs/","tags":["mkdocs","Github","Git"],"text":"D\u00e9ployer un site mkdocs via Github page Install python environnement/packages # Create Python venv python3 -m venv venv source venv/bin/activate # Install pip packages pip install --upgrade pip install mkdocs pip install mkdocs-material pip install mkdocs-git-revision-date-localized-plugin # Optional Setup Mkdocs mkdocs new GithubPage cd GithubPage/ # Configure git if you have multiple profile git config user.name \"XXXXXX\" git config user.email \"XXX@XXX.XXX\" # Check git configuration cat .git/config # Run mkdocs, URL => http://127.0.0.1:8000/ mkdocs serve # Build mkdocs webstire mkdocs build # Clone your empty github page project (public) git clone git@github.com:XXXXXX/XXXXX.github.io.git cd XXXXX.github.io/ git config user.name \"XXXXXX\" git config user.email \"XXX@XXX.XXX\" # Push your github page build with mkdocs mkdocs gh-deploy --config-file ../GithubPage/mkdocs.yml --remote-branch main","title":"mkdocs"},{"location":"Github/mkdocs/#deployer-un-site-mkdocs-via-github-page","text":"","title":"D\u00e9ployer un site mkdocs via Github page"},{"location":"Github/mkdocs/#install-python-environnementpackages","text":"# Create Python venv python3 -m venv venv source venv/bin/activate # Install pip packages pip install --upgrade pip install mkdocs pip install mkdocs-material pip install mkdocs-git-revision-date-localized-plugin # Optional","title":"Install python environnement/packages"},{"location":"Github/mkdocs/#setup-mkdocs","text":"mkdocs new GithubPage cd GithubPage/ # Configure git if you have multiple profile git config user.name \"XXXXXX\" git config user.email \"XXX@XXX.XXX\" # Check git configuration cat .git/config # Run mkdocs, URL => http://127.0.0.1:8000/ mkdocs serve # Build mkdocs webstire mkdocs build # Clone your empty github page project (public) git clone git@github.com:XXXXXX/XXXXX.github.io.git cd XXXXX.github.io/ git config user.name \"XXXXXX\" git config user.email \"XXX@XXX.XXX\" # Push your github page build with mkdocs mkdocs gh-deploy --config-file ../GithubPage/mkdocs.yml --remote-branch main","title":"Setup Mkdocs"},{"location":"Gitlab/config/","tags":["Gitlab"],"text":"Gitlab config Install yum -y install curl vim policycoreutils policycoreutils policycoreutils openssh-server perl postfix # Enable OpenSSH server daemon if not enabled: sudo systemctl status sshd systemctl enable sshd systemctl start sshd # Check if opening the firewall is needed with: sudo systemctl status firewalld firewall-cmd --permanent --add-service = http firewall-cmd --permanent --add-service = https systemctl reload firewalld firewall-cmd --list-all systemctl enable postfix && sudo systemctl start postfix curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash EXTERNAL_URL = \"https://192.168.1.XXX\" yum install -y gitlab-ce cat /etc/gitlab/initial_root_password Users Use cli gitlab-rails console -e production Find the user to unlock (by email or ID) user = User.find_by(email: 'admin@local.host') or user = User.where(id: 1).first Unlock the user user.unlock_access! Disable 2FA user.disable_two_factor! Tips /etc/gitlab/gitlab.rb Change external url external_url 'http://gitlab.example.com' Change gitlab datadir git_data_dirs({ \"default\" => { \"path\" => \"/PATH\" } }) Show config gitlab-ctl show-config Reload conf gitlab-ctl reconfigure gitlab-ctl restart What next ? Secure gitlab","title":"Config"},{"location":"Gitlab/config/#gitlab-config","text":"","title":"Gitlab config"},{"location":"Gitlab/config/#install","text":"yum -y install curl vim policycoreutils policycoreutils policycoreutils openssh-server perl postfix # Enable OpenSSH server daemon if not enabled: sudo systemctl status sshd systemctl enable sshd systemctl start sshd # Check if opening the firewall is needed with: sudo systemctl status firewalld firewall-cmd --permanent --add-service = http firewall-cmd --permanent --add-service = https systemctl reload firewalld firewall-cmd --list-all systemctl enable postfix && sudo systemctl start postfix curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash EXTERNAL_URL = \"https://192.168.1.XXX\" yum install -y gitlab-ce cat /etc/gitlab/initial_root_password","title":"Install"},{"location":"Gitlab/config/#users","text":"Use cli gitlab-rails console -e production Find the user to unlock (by email or ID) user = User.find_by(email: 'admin@local.host') or user = User.where(id: 1).first Unlock the user user.unlock_access! Disable 2FA user.disable_two_factor!","title":"Users"},{"location":"Gitlab/config/#tips","text":"/etc/gitlab/gitlab.rb Change external url external_url 'http://gitlab.example.com' Change gitlab datadir git_data_dirs({ \"default\" => { \"path\" => \"/PATH\" } }) Show config gitlab-ctl show-config","title":"Tips"},{"location":"Gitlab/config/#reload-conf","text":"gitlab-ctl reconfigure gitlab-ctl restart","title":"Reload conf"},{"location":"Gitlab/config/#what-next","text":"Secure gitlab","title":"What next ?"},{"location":"Linux/disk/","tags":["Linux","Partition"],"text":"Linux disk Add partition to linux Find your partition fdisk -l here my partition is /dev/sdb Create partition fdisk /dev/sdb List the partitions on the disk: p Delete partitions: d Re-create the partition: n (then primary(p) and enter (default settings)) List the partitions on the disk: p (sdb1 must appear) Save the changes: w Create directory (here: /data ) Create filesystem mkfs -t ext4 -L /data /dev/sdb1 Find partion UUID blkid /dev/sdb1 Add at the bottom of /etc/fstab UUID = e8eaa446-6b15-4a5e-8ba8-796a334ebcca /data ext4 defaults 0 0 Mount filesystem mount -a","title":"Disk"},{"location":"Linux/disk/#linux-disk","text":"","title":"Linux disk"},{"location":"Linux/disk/#add-partition-to-linux","text":"Find your partition fdisk -l here my partition is /dev/sdb Create partition fdisk /dev/sdb List the partitions on the disk: p Delete partitions: d Re-create the partition: n (then primary(p) and enter (default settings)) List the partitions on the disk: p (sdb1 must appear) Save the changes: w Create directory (here: /data ) Create filesystem mkfs -t ext4 -L /data /dev/sdb1 Find partion UUID blkid /dev/sdb1 Add at the bottom of /etc/fstab UUID = e8eaa446-6b15-4a5e-8ba8-796a334ebcca /data ext4 defaults 0 0 Mount filesystem mount -a","title":"Add partition to linux"},{"location":"Linux/network/","tags":["Linux","Network"],"text":"Linux network Disable IPv6 Edit /etc/sysctl.conf net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 Reload sysctl sysctl -p Linux static IP Edit /etc/sysconfig/network-scripts/ifcfg-ens18 Change BOOTPROTO value to none Apply network change Reboot","title":"Network"},{"location":"Linux/network/#linux-network","text":"","title":"Linux network"},{"location":"Linux/network/#disable-ipv6","text":"Edit /etc/sysctl.conf net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 Reload sysctl sysctl -p","title":"Disable IPv6"},{"location":"Linux/network/#linux-static-ip","text":"Edit /etc/sysconfig/network-scripts/ifcfg-ens18 Change BOOTPROTO value to none","title":"Linux static IP"},{"location":"Linux/network/#apply-network-change","text":"Reboot","title":"Apply network change"},{"location":"Virtualisation/Proxmox/LXC_pihole/","tags":["LXC","PiHole"],"text":"PiHole Setup Pihole from ubuntu LXC on proxmox Minimal requirements 4 Go DD 512 Mb RAM 1 VCP unprivileged start on boot Update/Upgrade LXC system Create new USER/GROUP on LXC adduser piholeadmin usermod -aG sudo piholeadmin Create a new ssh key on your local device ssh-keygen -t ed25519 Import your new public key to LXC Copy your public key to the LXC (you can't use ssh-copy-id piholeadmin@X.X.X.X here) On your LXC machine create a authorized_keys in your piholeadmin home PATH su piholeadmin cd ~ mkdir ~/.ssh touch ~/.ssh/authorized_keys sudo chmod 700 ~/.ssh sudo chmod 600 ~/.ssh/authorized_keys Then print the content of your local file ~/.ssh/id_ed25519.pub to ~/.ssh/authorized_keys on your LXC Configure SSH on LXC Basique conf of /etc/ssh/sshd_config (replace port and allowusers): Include /etc/ssh/sshd_config.d/*.conf Port XXXX PermitRootLogin no PasswordAuthentication no PermitEmptyPasswords no # Optionnal AllowUsers USER@X.X.X.* KbdInteractiveAuthentication no UsePAM yes X11Forwarding yes PrintMotd no AcceptEnv LANG LC_* Subsystem sftp /usr/lib/openssh/sftp-server Restart your SSH service service ssh restart Now you can connect your LXC with your piholeadmin user and your ssh key ssh -i ~.ssh/id_ed25519 -p { SSH pihole port } piholeadmin@ { pihole IP } Set your timezone timedatectl set-timezone Europe/Paris Install pihole apt install curl -y curl -sSL https://install.pi-hole.net | bash Change pihole password pihole -a -p ADD blocklist On pi-hole dashboard, go to \"Group Management > ADlists\" and paste your links (can paste multiple at once) https://firebog.net/ https://github.com/StevenBlack/hosts Reload your conf in \"tools > Update Gravity\"","title":"PiHole"},{"location":"Virtualisation/Proxmox/LXC_pihole/#pihole","text":"Setup Pihole from ubuntu LXC on proxmox","title":"PiHole"},{"location":"Virtualisation/Proxmox/LXC_pihole/#minimal-requirements","text":"4 Go DD 512 Mb RAM 1 VCP unprivileged start on boot Update/Upgrade LXC system","title":"Minimal requirements"},{"location":"Virtualisation/Proxmox/LXC_pihole/#create-new-usergroup-on-lxc","text":"adduser piholeadmin usermod -aG sudo piholeadmin","title":"Create new USER/GROUP on LXC"},{"location":"Virtualisation/Proxmox/LXC_pihole/#create-a-new-ssh-key-on-your-local-device","text":"ssh-keygen -t ed25519","title":"Create a new ssh key on your local device"},{"location":"Virtualisation/Proxmox/LXC_pihole/#import-your-new-public-key-to-lxc","text":"Copy your public key to the LXC (you can't use ssh-copy-id piholeadmin@X.X.X.X here) On your LXC machine create a authorized_keys in your piholeadmin home PATH su piholeadmin cd ~ mkdir ~/.ssh touch ~/.ssh/authorized_keys sudo chmod 700 ~/.ssh sudo chmod 600 ~/.ssh/authorized_keys Then print the content of your local file ~/.ssh/id_ed25519.pub to ~/.ssh/authorized_keys on your LXC","title":"Import your new public key to LXC"},{"location":"Virtualisation/Proxmox/LXC_pihole/#configure-ssh-on-lxc","text":"Basique conf of /etc/ssh/sshd_config (replace port and allowusers): Include /etc/ssh/sshd_config.d/*.conf Port XXXX PermitRootLogin no PasswordAuthentication no PermitEmptyPasswords no # Optionnal AllowUsers USER@X.X.X.* KbdInteractiveAuthentication no UsePAM yes X11Forwarding yes PrintMotd no AcceptEnv LANG LC_* Subsystem sftp /usr/lib/openssh/sftp-server Restart your SSH service service ssh restart Now you can connect your LXC with your piholeadmin user and your ssh key ssh -i ~.ssh/id_ed25519 -p { SSH pihole port } piholeadmin@ { pihole IP }","title":"Configure SSH on LXC"},{"location":"Virtualisation/Proxmox/LXC_pihole/#set-your-timezone","text":"timedatectl set-timezone Europe/Paris","title":"Set your timezone"},{"location":"Virtualisation/Proxmox/LXC_pihole/#install-pihole","text":"apt install curl -y curl -sSL https://install.pi-hole.net | bash Change pihole password pihole -a -p","title":"Install pihole"},{"location":"Virtualisation/Proxmox/LXC_pihole/#add-blocklist","text":"On pi-hole dashboard, go to \"Group Management > ADlists\" and paste your links (can paste multiple at once) https://firebog.net/ https://github.com/StevenBlack/hosts Reload your conf in \"tools > Update Gravity\"","title":"ADD blocklist"},{"location":"Virtualisation/Proxmox/prox_NFS/","tags":["Linux","Proxmox","NFS"],"text":"Proxmox NFS Context Dans un cluster de X nodes, 1 seul de ces nodes comporte un datastore Mise ne place ZFS Sur le noeud hebergeant le datastore, nous cr\u00e9ons un raid ZFS via proxmox Dashboard > node_name > Disks > ZFS Ici notre ZFS sera nomm\u00e9 Storage Confire NFS serveur service Sur ce m\u00eame noeud, nous configurons le server nfs via le shell Install package apt install nfs-kernel-server Edit /etc/exports /Storage 192.168.1.XXX(rw,no_root_squash,sync,no_subtree_check) /Storage 192.168.1.XXX(rw,no_root_squash,sync,no_subtree_check) Edit /etc/hosts.deny portmap:ALL nfsd:ALL mountd:ALL statd: ALL lockd: ALL rquotad: ALL rpcbind: ALL Edit /etc/hosts.allow portmap: 192 .168.1.XXXX lockd: 192 .168.1.XXXX nfsd: 192 .168.1.XXXX mountd: 192 .168.1.XXXX rquotad: 192 .168.1.XXXX statd: 192 .168.1.XXXX rpcbind: 192 .168.1.XXXX portmap: 192 .168.1.XXXX lockd: 192 .168.1.XXXX nfsd: 192 .168.1.XXXX mountd: 192 .168.1.XXXX rquotad: 192 .168.1.XXXX statd: 192 .168.1.XXXX rpcbind: 192 .168.1.XXXX Reload export config exportfs -a Restart nfs sevice service nfs-kernel-server restart Confire NFS client service Sur les autres noeud de notre cluster proxmox, on configure nfs client via le shell Install package apt install nfs-common Create directory (mount target) mkdir /Storage Edit fstab and add at the bottom (avec l'IP du noeud o\u00f9 l'on a configur\u00e9 le server NFS) 192 .168.1.XXX:/Storage /Storage nfs4 defaults,_netdev 0 0 mount nfs storage mount -a Create proxmox cluster directory proxmox dashboard > datacenter > Storage > add > directory Champ valeur ID nom que l'on souhaite donn\u00e9e au stockage Directory / Storage Content selection tout ce que vous souhaitez Shared check Enable check","title":"NFS"},{"location":"Virtualisation/Proxmox/prox_NFS/#proxmox-nfs","text":"","title":"Proxmox NFS"},{"location":"Virtualisation/Proxmox/prox_NFS/#context","text":"Dans un cluster de X nodes, 1 seul de ces nodes comporte un datastore","title":"Context"},{"location":"Virtualisation/Proxmox/prox_NFS/#mise-ne-place-zfs","text":"Sur le noeud hebergeant le datastore, nous cr\u00e9ons un raid ZFS via proxmox Dashboard > node_name > Disks > ZFS Ici notre ZFS sera nomm\u00e9 Storage","title":"Mise ne place ZFS"},{"location":"Virtualisation/Proxmox/prox_NFS/#confire-nfs-serveur-service","text":"Sur ce m\u00eame noeud, nous configurons le server nfs via le shell Install package apt install nfs-kernel-server Edit /etc/exports /Storage 192.168.1.XXX(rw,no_root_squash,sync,no_subtree_check) /Storage 192.168.1.XXX(rw,no_root_squash,sync,no_subtree_check) Edit /etc/hosts.deny portmap:ALL nfsd:ALL mountd:ALL statd: ALL lockd: ALL rquotad: ALL rpcbind: ALL Edit /etc/hosts.allow portmap: 192 .168.1.XXXX lockd: 192 .168.1.XXXX nfsd: 192 .168.1.XXXX mountd: 192 .168.1.XXXX rquotad: 192 .168.1.XXXX statd: 192 .168.1.XXXX rpcbind: 192 .168.1.XXXX portmap: 192 .168.1.XXXX lockd: 192 .168.1.XXXX nfsd: 192 .168.1.XXXX mountd: 192 .168.1.XXXX rquotad: 192 .168.1.XXXX statd: 192 .168.1.XXXX rpcbind: 192 .168.1.XXXX Reload export config exportfs -a Restart nfs sevice service nfs-kernel-server restart","title":"Confire NFS serveur service"},{"location":"Virtualisation/Proxmox/prox_NFS/#confire-nfs-client-service","text":"Sur les autres noeud de notre cluster proxmox, on configure nfs client via le shell Install package apt install nfs-common Create directory (mount target) mkdir /Storage Edit fstab and add at the bottom (avec l'IP du noeud o\u00f9 l'on a configur\u00e9 le server NFS) 192 .168.1.XXX:/Storage /Storage nfs4 defaults,_netdev 0 0 mount nfs storage mount -a","title":"Confire NFS client service"},{"location":"Virtualisation/Proxmox/prox_NFS/#create-proxmox-cluster-directory","text":"proxmox dashboard > datacenter > Storage > add > directory Champ valeur ID nom que l'on souhaite donn\u00e9e au stockage Directory / Storage Content selection tout ce que vous souhaitez Shared check Enable check","title":"Create proxmox cluster directory"},{"location":"Virtualisation/Proxmox/prox_packer/","tags":["Linux","Proxmox","packer"],"text":"Proxmox VM with packer packer can use json or hcl2, in this doc we will use json Requirements Packer / python (venv ?) / ansible (pip install ansible) Open firewall port on your packer device: firewall-cmd --zone=public --add-port=8000-9000/tcp --permanent then firewall-cmd --reload Generate proxmox api (id/token) Run packer (here from python venv) source venv/bin/activate cd packer/json packer build packer.json Use vault in your packer.json is way more secure than plain text (for all of your variable + ssh_username and ssh_password) like this example Folder structure | - json | | - http | | | - rocky.cfg | | - packer.json | | - playbook.yml Foler content rocky.cfg #version=RHEL8 ## Use graphical install ##graphical repo --name = \"AppStream\" --baseurl = file:///run/install/sources/mount-0000-cdrom/AppStream # Use text install text %packages @^minimal-environment kexec-tools %end # Keyboard layouts keyboard --xlayouts = 'fr (oss)' # System language lang fr_FR.UTF-8 # Network information network --bootproto = static --device = ens18 --gateway = 192 .168.1.XXX --ip = 192 .168.1.XXX --nameserver = 192 .168.1.XXX --netmask = 255 .255.255.0 --onboot = on --noipv6 --activate # Use CDROM installation media cdrom # Run the Setup Agent on first boot ##firstboot --enable firstboot --disabled # add eula --agreed ignoredisk --only-use = sda # Disk partitionning information ##part /boot --fstype=\"xfs\" --ondisk=sda --size=512 ##part pv.01 --fstype=\"lvmpv\" --ondisk=sda --grow ##volgroup vg_root --pesize=4096 pv.01 ##logvol /home --fstype=\"xfs\" --size=5120 --name=lv_home --vgname=vg_root ##logvol /var --fstype=\"xfs\" --size=10240 --name=lv_var --vgname=vg_root ##logvol / --fstype=\"xfs\" --size=10240 --name=lv_root --vgname=vg_root ##logvol swap --fstype=\"swap\" --size=4092 --name=lv_swap --vgname=vg_root autopart # Partition clearing information clearpart --all ##clearpart --none --initlabel # System timezone timezone Europe/Paris --isUtc --nontp # System services selinux --permissive firewall --enabled services --enabled = \"NetworkManager,sshd\" # Root password # rootpw --plaintext mypassword # can be find in a exist linux system (/etc/shadow) # can be generated, here with pythn3 => # >>> import crypt # >>> crypt.crypt(\"password\", crypt.mksalt()) rootpw --iscrypted $6$375 H8BSW9k1HCCpB $UmTXO6fNMKsc3zYIBq /R4fuFWi7zlAdLI4dvwqoCK8V4rL.THm4tFWHq/A/C8zXjiXUtqiPYAQDlcnvPgvCWI/ %addon com_redhat_kdump --disable reboot # Manage Ansible access ##groupadd -g 1001 ansible ##useradd -m -g 1001 -u 1001 ansible ##mkdir /home/ansible/.ssh ##echo -e \"<---- PAST YOUR PUBKEY HERE ---->\" > /home/ansible/.ssh/authorized_keys ##chown -R ansible:ansible /home/ansible/.ssh ##chmod 700 /home/ansible/.ssh ##chmod 600 /home/ansible/.ssh/authorized_keys ##echo \"ansible ALL=(ALL:ALL) NOPASSWD:ALL\" > /etc/sudoers.d/ansible ##chmod 440 /etc/sudoers.d/ansible %end packer.json { \"variables\" : { \"proxmox_node\" : \"master\" , \"proxmox_api_url\" : \"https://192.168.1.XX:8006/api2/json\" , \"proxmox_api_token_id\" : \"USER@pam!XXXX\" , \"proxmox_api_token_secret\" : \"XXX-XXX-XXXX-XXXX-XXXX\" , \"rocky_iso\" : \"StorageMount:iso/rocky-linux_8.6.iso\" }, \"builders\" : [ { \"type\" : \"proxmox\" , \"proxmox_url\" : \"{{user `proxmox_api_url`}}\" , \"insecure_skip_tls_verify\" : true , \"username\" : \"{{user `proxmox_api_token_id`}}\" , \"token\" : \"{{user `proxmox_api_token_secret`}}\" , \"node\" : \"{{user `proxmox_node`}}\" , \"vm_id\" : \"453\" , \"vm_name\" : \"test-rocky\" , \"qemu_agent\" : false , \"ssh_host\" : \"192.168.1.XX\" , \"network_adapters\" : [ { \"model\" : \"virtio\" , \"bridge\" : \"vmbr0\" , \"firewall\" : \"false\" } ], \"disks\" : [ { \"disk_size\" : \"10G\" , \"type\" : \"scsi\" , \"storage_pool\" : \"local-zfs\" , \"storage_pool_type\" : \"zfspool\" } ], \"cpu_type\" : \"host\" , \"cores\" : \"1\" , \"sockets\" : \"1\" , \"memory\" : \"1024\" , \"iso_file\" : \"{{user `rocky_iso`}}\" , \"unmount_iso\" : true , \"scsi_controller\" : \"virtio-scsi-pci\" , \"http_directory\" : \"http\" , \"boot_wait\" : \"10s\" , \"boot_command\" : [ \"<up><tab> text ip=dhcp nameserver=192.168.1.254 inst.ks=http://{{.HTTPIP}}:{{.HTTPPort}}/rocky.cfg<enter><wait><enter>\" ], \"ssh_username\" : \"root\" , \"ssh_timeout\" : \"15m\" , \"ssh_password\" : \"password\" } ], \"provisioners\" : [ { \"type\" : \"shell-local\" , \"inline\" : [ \"echo toto\" ] }, { \"type\" : \"ansible\" , \"user\" : \"root\" , \"extra_arguments\" : [ \"-v\" ], \"playbook_file\" : \"./playbook.yml\" } ] } playbook.yml --- - hosts : all become : yes pre_tasks : - name : Upgrade system before build dnf : name : '*' state : latest - name : Install epel release dnf : name : epel-release state : latest - name : Install common package dnf : name : \"{{ item }}\" state : present with_items : - vim - yum-utils","title":"Packer"},{"location":"Virtualisation/Proxmox/prox_packer/#proxmox-vm-with-packer","text":"packer can use json or hcl2, in this doc we will use json","title":"Proxmox VM with packer"},{"location":"Virtualisation/Proxmox/prox_packer/#requirements","text":"Packer / python (venv ?) / ansible (pip install ansible) Open firewall port on your packer device: firewall-cmd --zone=public --add-port=8000-9000/tcp --permanent then firewall-cmd --reload Generate proxmox api (id/token) Run packer (here from python venv) source venv/bin/activate cd packer/json packer build packer.json Use vault in your packer.json is way more secure than plain text (for all of your variable + ssh_username and ssh_password) like this example","title":"Requirements"},{"location":"Virtualisation/Proxmox/prox_packer/#folder-structure","text":"| - json | | - http | | | - rocky.cfg | | - packer.json | | - playbook.yml","title":"Folder structure"},{"location":"Virtualisation/Proxmox/prox_packer/#foler-content","text":"rocky.cfg #version=RHEL8 ## Use graphical install ##graphical repo --name = \"AppStream\" --baseurl = file:///run/install/sources/mount-0000-cdrom/AppStream # Use text install text %packages @^minimal-environment kexec-tools %end # Keyboard layouts keyboard --xlayouts = 'fr (oss)' # System language lang fr_FR.UTF-8 # Network information network --bootproto = static --device = ens18 --gateway = 192 .168.1.XXX --ip = 192 .168.1.XXX --nameserver = 192 .168.1.XXX --netmask = 255 .255.255.0 --onboot = on --noipv6 --activate # Use CDROM installation media cdrom # Run the Setup Agent on first boot ##firstboot --enable firstboot --disabled # add eula --agreed ignoredisk --only-use = sda # Disk partitionning information ##part /boot --fstype=\"xfs\" --ondisk=sda --size=512 ##part pv.01 --fstype=\"lvmpv\" --ondisk=sda --grow ##volgroup vg_root --pesize=4096 pv.01 ##logvol /home --fstype=\"xfs\" --size=5120 --name=lv_home --vgname=vg_root ##logvol /var --fstype=\"xfs\" --size=10240 --name=lv_var --vgname=vg_root ##logvol / --fstype=\"xfs\" --size=10240 --name=lv_root --vgname=vg_root ##logvol swap --fstype=\"swap\" --size=4092 --name=lv_swap --vgname=vg_root autopart # Partition clearing information clearpart --all ##clearpart --none --initlabel # System timezone timezone Europe/Paris --isUtc --nontp # System services selinux --permissive firewall --enabled services --enabled = \"NetworkManager,sshd\" # Root password # rootpw --plaintext mypassword # can be find in a exist linux system (/etc/shadow) # can be generated, here with pythn3 => # >>> import crypt # >>> crypt.crypt(\"password\", crypt.mksalt()) rootpw --iscrypted $6$375 H8BSW9k1HCCpB $UmTXO6fNMKsc3zYIBq /R4fuFWi7zlAdLI4dvwqoCK8V4rL.THm4tFWHq/A/C8zXjiXUtqiPYAQDlcnvPgvCWI/ %addon com_redhat_kdump --disable reboot # Manage Ansible access ##groupadd -g 1001 ansible ##useradd -m -g 1001 -u 1001 ansible ##mkdir /home/ansible/.ssh ##echo -e \"<---- PAST YOUR PUBKEY HERE ---->\" > /home/ansible/.ssh/authorized_keys ##chown -R ansible:ansible /home/ansible/.ssh ##chmod 700 /home/ansible/.ssh ##chmod 600 /home/ansible/.ssh/authorized_keys ##echo \"ansible ALL=(ALL:ALL) NOPASSWD:ALL\" > /etc/sudoers.d/ansible ##chmod 440 /etc/sudoers.d/ansible %end packer.json { \"variables\" : { \"proxmox_node\" : \"master\" , \"proxmox_api_url\" : \"https://192.168.1.XX:8006/api2/json\" , \"proxmox_api_token_id\" : \"USER@pam!XXXX\" , \"proxmox_api_token_secret\" : \"XXX-XXX-XXXX-XXXX-XXXX\" , \"rocky_iso\" : \"StorageMount:iso/rocky-linux_8.6.iso\" }, \"builders\" : [ { \"type\" : \"proxmox\" , \"proxmox_url\" : \"{{user `proxmox_api_url`}}\" , \"insecure_skip_tls_verify\" : true , \"username\" : \"{{user `proxmox_api_token_id`}}\" , \"token\" : \"{{user `proxmox_api_token_secret`}}\" , \"node\" : \"{{user `proxmox_node`}}\" , \"vm_id\" : \"453\" , \"vm_name\" : \"test-rocky\" , \"qemu_agent\" : false , \"ssh_host\" : \"192.168.1.XX\" , \"network_adapters\" : [ { \"model\" : \"virtio\" , \"bridge\" : \"vmbr0\" , \"firewall\" : \"false\" } ], \"disks\" : [ { \"disk_size\" : \"10G\" , \"type\" : \"scsi\" , \"storage_pool\" : \"local-zfs\" , \"storage_pool_type\" : \"zfspool\" } ], \"cpu_type\" : \"host\" , \"cores\" : \"1\" , \"sockets\" : \"1\" , \"memory\" : \"1024\" , \"iso_file\" : \"{{user `rocky_iso`}}\" , \"unmount_iso\" : true , \"scsi_controller\" : \"virtio-scsi-pci\" , \"http_directory\" : \"http\" , \"boot_wait\" : \"10s\" , \"boot_command\" : [ \"<up><tab> text ip=dhcp nameserver=192.168.1.254 inst.ks=http://{{.HTTPIP}}:{{.HTTPPort}}/rocky.cfg<enter><wait><enter>\" ], \"ssh_username\" : \"root\" , \"ssh_timeout\" : \"15m\" , \"ssh_password\" : \"password\" } ], \"provisioners\" : [ { \"type\" : \"shell-local\" , \"inline\" : [ \"echo toto\" ] }, { \"type\" : \"ansible\" , \"user\" : \"root\" , \"extra_arguments\" : [ \"-v\" ], \"playbook_file\" : \"./playbook.yml\" } ] } playbook.yml --- - hosts : all become : yes pre_tasks : - name : Upgrade system before build dnf : name : '*' state : latest - name : Install epel release dnf : name : epel-release state : latest - name : Install common package dnf : name : \"{{ item }}\" state : present with_items : - vim - yum-utils","title":"Foler content"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/","tags":["Bash","VMWmare"],"text":"Start/Stop VM via Api Vmware Dans l'optique d'optimiser les coups dans l'IT(Finops), on peut \u00eatre amen\u00e9 \u00e0 mettre en place un arr\u00eat de l'environnement de qualifications les soirs et weekend par exemple. Afin de pouvoir v\u00e9rifier plus facilement la prise en compte de cette partie, on passera par l'affectation d'un tag sp\u00e9cifique pour l'ensemble des VM que l'on souhaite int\u00e9grer \u00e0 ce syst\u00e8me. Ils seront alors directement visible dans l'interface graphique de vmware. On partira du postula que l'ensemble des scripts comporte des variables user/password Dans les morceaux de scripts ci-apr\u00e8s, l'option curl k est pr\u00e9sente car mon serveur VMware ne dispose pas de certificat valide Pr\u00e9requis Un utilisateur ou token avec les bons droits d'utilisation de l'api vmware Cr\u00e9er un nouveau tag vmware R\u00e9cup\u00e9r\u00e9 notre tag Une fois votre tag cr\u00e9\u00e9, il faut r\u00e9cup\u00e9rer son ID. ## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Liste des ID tags tags_id = $( curl -skX GET -H \"vmware-api-session-id: $id_session \" -H 'Accept: application/json' https:// $vcenter /rest/com/vmware/cis/tagging/tag | jq -r .value [] ) ## Liste des ID tags avec leur nom ## Si le premier parametre du script est ## - all => on affiche toute la liste ## - le nom de notre nouveau tag => on affiche son ID if [ \" $1 \" == \"all\" ] then for tag in $tags_id do curl -skX GET -H \"vmware-api-session-id: $id_session \" -H 'Accept: application/json' https:// $vcenter /rest/com/vmware/cis/tagging/tag/id: $tag | jq -r '.value | \"tag_name: \\(.name)\\t\\ttag_id: \\(.id)\"' done elif [ \" $1 \" ! = \"\" ] then for tag in $tags_id do curl -skX GET -H \"vmware-api-session-id: $id_session \" -H 'Accept: application/json' https:// $vcenter /rest/com/vmware/cis/tagging/tag/id: $tag | jq -r '.value | \"tag_name: \\(.name)\\t\\ttag_id: \\(.id)\"' | grep -E $1 done fi Ajouter un tag \u00e0 une VM A partir d'ici, on mettra l'ID du nouveau tag dans les scripts. Il se pr\u00e9sente sous la forme : urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL ## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Utilisation de notre tag tag_id = 'urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL' ## Liste des VM \u00e0 downscale \u00e0 partie de leur nom ## On utilisera le pipe comme d\u00e9limiteur vm_a_tag = \"vm1|vm2|vm3\" ## Recuperation des ID de VM vm_name_id = $( curl -skX GET -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenter /rest/vcenter/vm/ | jq -r '.value[] | \"\\(.name),\\(.vm)\"' | grep -E \" $vm_a_tag \" ) ## Ajout du tag aux VM de la liste for vm in $vm_name_id do vm_id = \" $( echo $vm | cut -d, -f2 ) \" echo \"Ajout du tag \u00e0 la VM $( echo $vm | cut -d, -f1 ) \" curl -skX POST -H \"vmware-api-session-id: $id_session \" -H 'Content-Type: application/json' -d '{\"object_id\": { \"id\": \"' $vm_id '\", \"type\": \"VirtualMachine\"} }' https:// $vcenter /rest/com/vmware/cis/tagging/tag-association/id: { $tag_id } ?~action = attach done Pour retirer le tag d'une ou plusieurs VM, remplacer simplement attach par detach dans le script au dessus (dernier appel curl) et relancer le Liste des VM avec le nouveau tag ## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Utilisation de notre tag tag_id = 'urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL' ## Liste des ID des VM avec notre tag vm_tagged = $( curl -skX POST -H \"vmware-api-session-id: $id_session \" -H 'Content-Type: application/json' -d '{\"tag_ids\": [\"' $tag_id '\"]}' https:// $vcenter /rest/com/vmware/cis/tagging/tag-association?~action = list-attached-objects-on-tags | jq -r .value [] .object_ids [] .id ) ## Liste des nom des VM avec notre tag for id_vm in $vm_tagged do vm_name = $( curl -skX GET -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenter /rest/vcenter/vm/ $id_vm | jq -r .value.name ) echo \" $vm_name ( $id_vm )\" done Allumer/\u00c9teindre les VM avec le tag ## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Utilisation de notre tag tag_id = 'urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL' ## Liste des ID des VM avec notre tag vm_tagged = $( curl -skX POST -H \"vmware-api-session-id: $id_session \" -H 'Content-Type: application/json' -d '{\"tag_ids\": [\"' $tag_id '\"]}' https:// $vcenter /rest/com/vmware/cis/tagging/tag-association?~action = list-attached-objects-on-tags | jq -r .value [] .object_ids [] .id ) ## Si le premier parametre du script est ## - stop => on eteint la VM ## - start => on allume la VM for id_vm in $vm_tagged do if [[ $1 == \"stop\" ]] then curl -skX POST -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenterr /rest/vcenter/vm/ $id_vm /power/stop fi if [[ $1 == \"upscale\" ]] then curl -skX POST -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenter /rest/vcenter/vm/ $id_vm /power/start fi if [[ $? -ne 0 ]] then echo \"Error\" fi done Une fois cr\u00e9\u00e9 et test\u00e9, ce script peux \u00eatre mis dans une crontab ou systemd-timer Plus de d\u00e9tails developer.vmware.com","title":"Start/Stop VM via Api Vmware"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/#startstop-vm-via-api-vmware","text":"Dans l'optique d'optimiser les coups dans l'IT(Finops), on peut \u00eatre amen\u00e9 \u00e0 mettre en place un arr\u00eat de l'environnement de qualifications les soirs et weekend par exemple. Afin de pouvoir v\u00e9rifier plus facilement la prise en compte de cette partie, on passera par l'affectation d'un tag sp\u00e9cifique pour l'ensemble des VM que l'on souhaite int\u00e9grer \u00e0 ce syst\u00e8me. Ils seront alors directement visible dans l'interface graphique de vmware. On partira du postula que l'ensemble des scripts comporte des variables user/password Dans les morceaux de scripts ci-apr\u00e8s, l'option curl k est pr\u00e9sente car mon serveur VMware ne dispose pas de certificat valide","title":"Start/Stop VM via Api Vmware"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/#prerequis","text":"Un utilisateur ou token avec les bons droits d'utilisation de l'api vmware Cr\u00e9er un nouveau tag vmware","title":"Pr\u00e9requis"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/#recupere-notre-tag","text":"Une fois votre tag cr\u00e9\u00e9, il faut r\u00e9cup\u00e9rer son ID. ## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Liste des ID tags tags_id = $( curl -skX GET -H \"vmware-api-session-id: $id_session \" -H 'Accept: application/json' https:// $vcenter /rest/com/vmware/cis/tagging/tag | jq -r .value [] ) ## Liste des ID tags avec leur nom ## Si le premier parametre du script est ## - all => on affiche toute la liste ## - le nom de notre nouveau tag => on affiche son ID if [ \" $1 \" == \"all\" ] then for tag in $tags_id do curl -skX GET -H \"vmware-api-session-id: $id_session \" -H 'Accept: application/json' https:// $vcenter /rest/com/vmware/cis/tagging/tag/id: $tag | jq -r '.value | \"tag_name: \\(.name)\\t\\ttag_id: \\(.id)\"' done elif [ \" $1 \" ! = \"\" ] then for tag in $tags_id do curl -skX GET -H \"vmware-api-session-id: $id_session \" -H 'Accept: application/json' https:// $vcenter /rest/com/vmware/cis/tagging/tag/id: $tag | jq -r '.value | \"tag_name: \\(.name)\\t\\ttag_id: \\(.id)\"' | grep -E $1 done fi","title":"R\u00e9cup\u00e9r\u00e9 notre tag"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/#ajouter-un-tag-a-une-vm","text":"A partir d'ici, on mettra l'ID du nouveau tag dans les scripts. Il se pr\u00e9sente sous la forme : urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL ## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Utilisation de notre tag tag_id = 'urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL' ## Liste des VM \u00e0 downscale \u00e0 partie de leur nom ## On utilisera le pipe comme d\u00e9limiteur vm_a_tag = \"vm1|vm2|vm3\" ## Recuperation des ID de VM vm_name_id = $( curl -skX GET -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenter /rest/vcenter/vm/ | jq -r '.value[] | \"\\(.name),\\(.vm)\"' | grep -E \" $vm_a_tag \" ) ## Ajout du tag aux VM de la liste for vm in $vm_name_id do vm_id = \" $( echo $vm | cut -d, -f2 ) \" echo \"Ajout du tag \u00e0 la VM $( echo $vm | cut -d, -f1 ) \" curl -skX POST -H \"vmware-api-session-id: $id_session \" -H 'Content-Type: application/json' -d '{\"object_id\": { \"id\": \"' $vm_id '\", \"type\": \"VirtualMachine\"} }' https:// $vcenter /rest/com/vmware/cis/tagging/tag-association/id: { $tag_id } ?~action = attach done Pour retirer le tag d'une ou plusieurs VM, remplacer simplement attach par detach dans le script au dessus (dernier appel curl) et relancer le","title":"Ajouter un tag \u00e0 une VM"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/#liste-des-vm-avec-le-nouveau-tag","text":"## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Utilisation de notre tag tag_id = 'urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL' ## Liste des ID des VM avec notre tag vm_tagged = $( curl -skX POST -H \"vmware-api-session-id: $id_session \" -H 'Content-Type: application/json' -d '{\"tag_ids\": [\"' $tag_id '\"]}' https:// $vcenter /rest/com/vmware/cis/tagging/tag-association?~action = list-attached-objects-on-tags | jq -r .value [] .object_ids [] .id ) ## Liste des nom des VM avec notre tag for id_vm in $vm_tagged do vm_name = $( curl -skX GET -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenter /rest/vcenter/vm/ $id_vm | jq -r .value.name ) echo \" $vm_name ( $id_vm )\" done","title":"Liste des VM avec le nouveau tag"},{"location":"Virtualisation/VMWare/start_stop_vm_api_vmware/#allumereteindre-les-vm-avec-le-tag","text":"## URL de votre Vcenter vcenter = 'XXXXX.XXXX.XX' ## Creation de votre session \u00e0 l'api Vmware id_session = $( curl -skX POST https:// $vcenter /rest/com/vmware/cis/session -u $( echo $user ) : $( echo $password ) | jq -r .value ) ## Utilisation de notre tag tag_id = 'urn:vmomi:InventoryServiceTag:XXXXX-XXXXX-XXXXX:GLOBAL' ## Liste des ID des VM avec notre tag vm_tagged = $( curl -skX POST -H \"vmware-api-session-id: $id_session \" -H 'Content-Type: application/json' -d '{\"tag_ids\": [\"' $tag_id '\"]}' https:// $vcenter /rest/com/vmware/cis/tagging/tag-association?~action = list-attached-objects-on-tags | jq -r .value [] .object_ids [] .id ) ## Si le premier parametre du script est ## - stop => on eteint la VM ## - start => on allume la VM for id_vm in $vm_tagged do if [[ $1 == \"stop\" ]] then curl -skX POST -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenterr /rest/vcenter/vm/ $id_vm /power/stop fi if [[ $1 == \"upscale\" ]] then curl -skX POST -H \"vmware-api-session-id: $id_session \" -H \"Accept: application/json\" https:// $vcenter /rest/vcenter/vm/ $id_vm /power/start fi if [[ $? -ne 0 ]] then echo \"Error\" fi done Une fois cr\u00e9\u00e9 et test\u00e9, ce script peux \u00eatre mis dans une crontab ou systemd-timer Plus de d\u00e9tails developer.vmware.com","title":"Allumer/\u00c9teindre les VM avec le tag"},{"location":"Zabbix/zabbix_soft_discovery/","tags":["Zabbix","Monitoring"],"text":"Pr\u00e9sentation Zabbix est un puissant outil de monitoring. La mise en place de l'auto discovery Zabbix est tr\u00e8s pratique afin de r\u00e9cup\u00e9rer des donn\u00e9es remont\u00e9es dynamiquement par des API par exemple. Le probl\u00e8me qui peut survenir lors de sa mise en place est que l'on peut saturer aussi bien le r\u00e9seau, que notre API suivant le nombre de donn\u00e9es que l'on souhaite r\u00e9cup\u00e9rer. Context Nous avons un service rabbitmq qui remonte ces metrics dans un prometheus. Depuis Zabbix, nous voulons mettre en place un auto discovery afin de r\u00e9cup\u00e9rer l'ensemble des messages pr\u00e9sents dans les diff\u00e9rentes queues via le prometheus. Nous passons par de l'auto discovery afin que les futures queues cr\u00e9\u00e9s dans rabbitmq remontent automatiquement dans zabbix (de m\u00eame pour celles qui seront supprim\u00e9es) Mise en place de l'Auto Discovery Depuis notre Host zabbix, dans la partie discovery, nous cr\u00e9ons une \"discovery rule\". Celle-ci nous permettra de r\u00e9cup\u00e9rer l'ensemble des noms des queues pr\u00e9sentes dans le rabbitmq. On lui donne: Un nom Un type => ici \"HTTP agent\" Un cl\u00e9 (key) unique (exemple rabbitmq.autodiscovery.queues) L'Url de notre prometheus (je vous conseil fortement de passer par une Macro zabbix) La query (qui s'ex\u00e9cutera sur le prometheus) => ici on r\u00e9cup\u00e8re les messages des queues rabbitmq Un intervale de mise \u00e0 jour (par d\u00e9faut 1 minutes) Une dur\u00e9e de retention des resources plus pr\u00e9sent (30 jour par d\u00e9faut, diminuer le \u00e0 1 jour) A ce moment, si l'on test notre Discovery rule, on constate que nous avons un retour Json sous la forme: { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [{ \"metric\" : { \"queue\" : \"queue_name1\" } , \"value\" : [ XXXXXXXXXX.XXX, \"1\" ]} , { \"metric\" : { \"queue\" : \"queue_name2\" } , \"value\" : [ XXXXXXXXXX.XXX, \"2\" ]} , { \"metric\" : { \"queue\" : \"queue_name3\" } , \"value\" : [ XXXXXXXXXX.XXX, \"3\" ]}] Les informations qui nous int\u00e9ressent ici sont pr\u00e9sentes dans \"metric\". Nous allons donc les r\u00e9cup\u00e9rer dans le \"Preprocessing\" de notre discovery rule. Nous avons maintenant uniquement queue dans notre retour Json sous la forme: [{ \"queue\" : \"queue_name1\" } , { \"queue\" : \"queue_name2\" } , { \"queue\" : \"queue_name3\" }] Gr\u00e2ce \u00e0 macro de d\u00e9couverte Zabbix (LLD), nous pouvons r\u00e9cup\u00e9rer le noms de chaque queue pr\u00e9sent dans le Path \"queue\". Par la suite, si vous souhaitez ne pas r\u00e9cup\u00e9rer certaines queue ou \u00e0 l'inverse n'en r\u00e9cup\u00e9rer que certaines sp\u00e9cifique, vous pouvez les filtrer Items prototypes \"classique\" L'item prototypes ce base sur la macro LLD r\u00e9cup\u00e9rer par la discovery rule auquel il est associ\u00e9e (pour nous {#QUEUE}). On lui donne: Un nom Un type => ici \"HTTP agent\" Un cl\u00e9 (key) unique gr\u00e2ce \u00e0 la macro LLD (exmple: rabbitmq.items.queue.[{#QUEUE}]) L'Url de notre prometheus La query (qui s'ex\u00e9cutera sur le prometheus) Un intervale de mise \u00e0 jour (par d\u00e9faut 1 minutes) Pour cet item prototype, Type of information => Numeric (unsinged) Notre retour Json sera alors sous la forme: { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [{ \"metric\" : { \"queue\" : \"queue_name1\" } , \"value\" : [ XXXXXXXXXX.XXX, \"1\" ]} , { \"metric\" : { \"queue\" : \"queue_name2\" } , \"value\" : [ XXXXXXXXXX.XXX, \"2\" ]} , { \"metric\" : { \"queue\" : \"queue_name3\" } , \"value\" : [ XXXXXXXXXX.XXX, \"3\" ]}] On passe donc par un nouveau preprocessing afin de r\u00e9cup\u00e9rer uniquement les valeurs qui correspondent au nombre de messages dans nos queues rabbitmq (dans notre exemple, les valeurs sont respectivement 1,2 et 3) Une fois notre configuration sauvegard\u00e9, nos items (ici 3) vont se cr\u00e9er automatiquement avec les noms: Message in queue: queue_name1 Message in queue: queue_name2 Message in queue: queue_name3 Bien que cela soit parfaitement fonctionnelle, il faut bien comprendre que nous effectuons 4 appels au prometheus toutes les minutes. Un appel pour r\u00e9cup\u00e9rer: Le nom de l'ensemble des queues Le nombre de message dans le queue_name1 Le nombre de message dans le queue_name2 Le nombre de message dans le queue_name3 Maintenant si l'on devait r\u00e9cup\u00e9rer le nombre de message sur 1000 queues rabbitmq diff\u00e9rentes, on arriverait \u00e0 1001 req/min. Les performances aussi bien de votre r\u00e9seau que celles de votre prometheus pourraient en \u00eatre fortement impact\u00e9es Optimisation des items prototypes Afin d'optimiser le nombre d'appel des items prototypes, nous allons utiliser les d\u00e9pendances d'items Zabbix. La partie discovery rules ne change pas. Dans un premier temps nous allons cr\u00e9er un item ( non prototypes ). Il fera la m\u00eame requ\u00eate que notre discovery rule. Pour cet item, Type of information => Text Comme ce type d'item peut r\u00e9cup\u00e9rer des donn\u00e9es assez volumineuses, je vous conseil de limiter le stockage de son historique (History storage period) sur 1 jour voir le d\u00e9sactiver Cette fois ci, notre Preprocessing se fera sur l'ensemble de nos valeur dans le Json path data.result Les donn\u00e9es de cet item seront sous la forme: [{ \"metric\" : { \"queue\" : \"queue_name1\" } , \"value\" : [ XXXXXXXXXX.XXX, \"1\" ]} , { \"metric\" : { \"queue\" : \"queue_name2\" } , \"value\" : [ XXXXXXXXXX.XXX, \"2\" ]} , { \"metric\" : { \"queue\" : \"queue_name3\" } , \"value\" : [ XXXXXXXXXX.XXX, \"3\" ]}] Retournons ensuite dans notre discovery rule afin d'y cr\u00e9er un item prototype. Comme pr\u00e9cis\u00e9 en d\u00e9but de section, nous utiliserons de de Type \"Dependent item\" qui fera r\u00e9f\u00e9rence \u00e0 l'item que nous venons de cr\u00e9er. Pour cet item prototype, Type of information => Numeric (unsinged) (DIGITAL \u00e9tant dans cette exemple le nom de mon Host Zabbix) Maintenant que notre item prototypes s'appuis sur les donn\u00e9es r\u00e9cup\u00e9rer par notre item et la macro LLD de notre discovery rule qui r\u00e9cup\u00e8re la liste des nom des queue rabbitmq, il ne nous reste plus qu'\u00e0 faire un dernier preprocessing afin d'avoir les valeurs queue par queue. A l'aide de ce param\u00e9trage, nous avons optimiser notre nombre de requ\u00eates au prometheus afin de r\u00e9cup\u00e9rer le nombre de messages par queue rabbitmq. Vis \u00e0 vis du premier exemple que j'ai nomm\u00e9 \"classique\", nous n'effectuons plus que 2 requ\u00eates quelque soit le cas de figure (1 queue comme 10000 !)","title":"Limit autodiscovery network calls"},{"location":"Zabbix/zabbix_soft_discovery/#presentation","text":"Zabbix est un puissant outil de monitoring. La mise en place de l'auto discovery Zabbix est tr\u00e8s pratique afin de r\u00e9cup\u00e9rer des donn\u00e9es remont\u00e9es dynamiquement par des API par exemple. Le probl\u00e8me qui peut survenir lors de sa mise en place est que l'on peut saturer aussi bien le r\u00e9seau, que notre API suivant le nombre de donn\u00e9es que l'on souhaite r\u00e9cup\u00e9rer.","title":"Pr\u00e9sentation"},{"location":"Zabbix/zabbix_soft_discovery/#context","text":"Nous avons un service rabbitmq qui remonte ces metrics dans un prometheus. Depuis Zabbix, nous voulons mettre en place un auto discovery afin de r\u00e9cup\u00e9rer l'ensemble des messages pr\u00e9sents dans les diff\u00e9rentes queues via le prometheus. Nous passons par de l'auto discovery afin que les futures queues cr\u00e9\u00e9s dans rabbitmq remontent automatiquement dans zabbix (de m\u00eame pour celles qui seront supprim\u00e9es)","title":"Context"},{"location":"Zabbix/zabbix_soft_discovery/#mise-en-place-de-lauto-discovery","text":"Depuis notre Host zabbix, dans la partie discovery, nous cr\u00e9ons une \"discovery rule\". Celle-ci nous permettra de r\u00e9cup\u00e9rer l'ensemble des noms des queues pr\u00e9sentes dans le rabbitmq. On lui donne: Un nom Un type => ici \"HTTP agent\" Un cl\u00e9 (key) unique (exemple rabbitmq.autodiscovery.queues) L'Url de notre prometheus (je vous conseil fortement de passer par une Macro zabbix) La query (qui s'ex\u00e9cutera sur le prometheus) => ici on r\u00e9cup\u00e8re les messages des queues rabbitmq Un intervale de mise \u00e0 jour (par d\u00e9faut 1 minutes) Une dur\u00e9e de retention des resources plus pr\u00e9sent (30 jour par d\u00e9faut, diminuer le \u00e0 1 jour) A ce moment, si l'on test notre Discovery rule, on constate que nous avons un retour Json sous la forme: { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [{ \"metric\" : { \"queue\" : \"queue_name1\" } , \"value\" : [ XXXXXXXXXX.XXX, \"1\" ]} , { \"metric\" : { \"queue\" : \"queue_name2\" } , \"value\" : [ XXXXXXXXXX.XXX, \"2\" ]} , { \"metric\" : { \"queue\" : \"queue_name3\" } , \"value\" : [ XXXXXXXXXX.XXX, \"3\" ]}] Les informations qui nous int\u00e9ressent ici sont pr\u00e9sentes dans \"metric\". Nous allons donc les r\u00e9cup\u00e9rer dans le \"Preprocessing\" de notre discovery rule. Nous avons maintenant uniquement queue dans notre retour Json sous la forme: [{ \"queue\" : \"queue_name1\" } , { \"queue\" : \"queue_name2\" } , { \"queue\" : \"queue_name3\" }] Gr\u00e2ce \u00e0 macro de d\u00e9couverte Zabbix (LLD), nous pouvons r\u00e9cup\u00e9rer le noms de chaque queue pr\u00e9sent dans le Path \"queue\". Par la suite, si vous souhaitez ne pas r\u00e9cup\u00e9rer certaines queue ou \u00e0 l'inverse n'en r\u00e9cup\u00e9rer que certaines sp\u00e9cifique, vous pouvez les filtrer","title":"Mise en place de l'Auto Discovery"},{"location":"Zabbix/zabbix_soft_discovery/#items-prototypes-classique","text":"L'item prototypes ce base sur la macro LLD r\u00e9cup\u00e9rer par la discovery rule auquel il est associ\u00e9e (pour nous {#QUEUE}). On lui donne: Un nom Un type => ici \"HTTP agent\" Un cl\u00e9 (key) unique gr\u00e2ce \u00e0 la macro LLD (exmple: rabbitmq.items.queue.[{#QUEUE}]) L'Url de notre prometheus La query (qui s'ex\u00e9cutera sur le prometheus) Un intervale de mise \u00e0 jour (par d\u00e9faut 1 minutes) Pour cet item prototype, Type of information => Numeric (unsinged) Notre retour Json sera alors sous la forme: { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [{ \"metric\" : { \"queue\" : \"queue_name1\" } , \"value\" : [ XXXXXXXXXX.XXX, \"1\" ]} , { \"metric\" : { \"queue\" : \"queue_name2\" } , \"value\" : [ XXXXXXXXXX.XXX, \"2\" ]} , { \"metric\" : { \"queue\" : \"queue_name3\" } , \"value\" : [ XXXXXXXXXX.XXX, \"3\" ]}] On passe donc par un nouveau preprocessing afin de r\u00e9cup\u00e9rer uniquement les valeurs qui correspondent au nombre de messages dans nos queues rabbitmq (dans notre exemple, les valeurs sont respectivement 1,2 et 3) Une fois notre configuration sauvegard\u00e9, nos items (ici 3) vont se cr\u00e9er automatiquement avec les noms: Message in queue: queue_name1 Message in queue: queue_name2 Message in queue: queue_name3 Bien que cela soit parfaitement fonctionnelle, il faut bien comprendre que nous effectuons 4 appels au prometheus toutes les minutes. Un appel pour r\u00e9cup\u00e9rer: Le nom de l'ensemble des queues Le nombre de message dans le queue_name1 Le nombre de message dans le queue_name2 Le nombre de message dans le queue_name3 Maintenant si l'on devait r\u00e9cup\u00e9rer le nombre de message sur 1000 queues rabbitmq diff\u00e9rentes, on arriverait \u00e0 1001 req/min. Les performances aussi bien de votre r\u00e9seau que celles de votre prometheus pourraient en \u00eatre fortement impact\u00e9es","title":"Items prototypes \"classique\""},{"location":"Zabbix/zabbix_soft_discovery/#optimisation-des-items-prototypes","text":"Afin d'optimiser le nombre d'appel des items prototypes, nous allons utiliser les d\u00e9pendances d'items Zabbix. La partie discovery rules ne change pas. Dans un premier temps nous allons cr\u00e9er un item ( non prototypes ). Il fera la m\u00eame requ\u00eate que notre discovery rule. Pour cet item, Type of information => Text Comme ce type d'item peut r\u00e9cup\u00e9rer des donn\u00e9es assez volumineuses, je vous conseil de limiter le stockage de son historique (History storage period) sur 1 jour voir le d\u00e9sactiver Cette fois ci, notre Preprocessing se fera sur l'ensemble de nos valeur dans le Json path data.result Les donn\u00e9es de cet item seront sous la forme: [{ \"metric\" : { \"queue\" : \"queue_name1\" } , \"value\" : [ XXXXXXXXXX.XXX, \"1\" ]} , { \"metric\" : { \"queue\" : \"queue_name2\" } , \"value\" : [ XXXXXXXXXX.XXX, \"2\" ]} , { \"metric\" : { \"queue\" : \"queue_name3\" } , \"value\" : [ XXXXXXXXXX.XXX, \"3\" ]}] Retournons ensuite dans notre discovery rule afin d'y cr\u00e9er un item prototype. Comme pr\u00e9cis\u00e9 en d\u00e9but de section, nous utiliserons de de Type \"Dependent item\" qui fera r\u00e9f\u00e9rence \u00e0 l'item que nous venons de cr\u00e9er. Pour cet item prototype, Type of information => Numeric (unsinged) (DIGITAL \u00e9tant dans cette exemple le nom de mon Host Zabbix) Maintenant que notre item prototypes s'appuis sur les donn\u00e9es r\u00e9cup\u00e9rer par notre item et la macro LLD de notre discovery rule qui r\u00e9cup\u00e8re la liste des nom des queue rabbitmq, il ne nous reste plus qu'\u00e0 faire un dernier preprocessing afin d'avoir les valeurs queue par queue. A l'aide de ce param\u00e9trage, nous avons optimiser notre nombre de requ\u00eates au prometheus afin de r\u00e9cup\u00e9rer le nombre de messages par queue rabbitmq. Vis \u00e0 vis du premier exemple que j'ai nomm\u00e9 \"classique\", nous n'effectuons plus que 2 requ\u00eates quelque soit le cas de figure (1 queue comme 10000 !)","title":"Optimisation des items prototypes"},{"location":"tags/","text":"Tags Following is a list of relevant tags: Bash Start/Stop VM via Api Vmware Git Deploy Hugo webpages on Github Deploy mkdocs webpages on Github Github Deploy Hugo webpages on Github Deploy mkdocs webpages on Github Gitlab Gitlab configuration LXC PiHole Linux Linux manage disk Linux manage network Proxmox NFS Proxmox packer Monitoring Limiter les appels de l'auto discovery Zabbix NFS Proxmox NFS Network Linux manage network Partition Linux manage disk PiHole PiHole Proxmox Proxmox NFS Proxmox packer VMWmare Start/Stop VM via Api Vmware Zabbix Limiter les appels de l'auto discovery Zabbix goHugo Deploy Hugo webpages on Github mkdocs Deploy mkdocs webpages on Github packer Proxmox packer","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#bash","text":"Start/Stop VM via Api Vmware","title":"Bash"},{"location":"tags/#git","text":"Deploy Hugo webpages on Github Deploy mkdocs webpages on Github","title":"Git"},{"location":"tags/#github","text":"Deploy Hugo webpages on Github Deploy mkdocs webpages on Github","title":"Github"},{"location":"tags/#gitlab","text":"Gitlab configuration","title":"Gitlab"},{"location":"tags/#lxc","text":"PiHole","title":"LXC"},{"location":"tags/#linux","text":"Linux manage disk Linux manage network Proxmox NFS Proxmox packer","title":"Linux"},{"location":"tags/#monitoring","text":"Limiter les appels de l'auto discovery Zabbix","title":"Monitoring"},{"location":"tags/#nfs","text":"Proxmox NFS","title":"NFS"},{"location":"tags/#network","text":"Linux manage network","title":"Network"},{"location":"tags/#partition","text":"Linux manage disk","title":"Partition"},{"location":"tags/#pihole","text":"PiHole","title":"PiHole"},{"location":"tags/#proxmox","text":"Proxmox NFS Proxmox packer","title":"Proxmox"},{"location":"tags/#vmwmare","text":"Start/Stop VM via Api Vmware","title":"VMWmare"},{"location":"tags/#zabbix","text":"Limiter les appels de l'auto discovery Zabbix","title":"Zabbix"},{"location":"tags/#gohugo","text":"Deploy Hugo webpages on Github","title":"goHugo"},{"location":"tags/#mkdocs","text":"Deploy mkdocs webpages on Github","title":"mkdocs"},{"location":"tags/#packer","text":"Proxmox packer","title":"packer"}]}